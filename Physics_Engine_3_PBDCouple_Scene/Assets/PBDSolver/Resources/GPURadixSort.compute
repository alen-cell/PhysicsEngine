//#include "Utility.compute"
// Create a RenderTexture with enableRandomWrite flag and set it
#pragma kernel CheckOrder
#pragma kernel LocalCount
#pragma kernel PrecedeScan
#pragma kernel LocalSort
#pragma kernel ComputeGroupKeyOffset
#pragma kernel GlobalReorder

#define SORT_SECTION_SIZE 8
#define SORT_SECTION_SIZE_2D 64
#define LOG2_SORT_SECTION_SIZE_2D 6
#define BITS_PER_ROTATION 4
#define BITS_MASK 15
#define BUCKET_SIZE 16

//Global Data Init
uniform uint _ParticleNum;
uint _SortSectionNum;
uint _RotationRound;
StructuredBuffer<uint2> _IndexMap;
RWBuffer<uint> _Ordered;
RWBuffer<uint>_LocalBinMarkers;
RWBuffer<uint>_GlobalPrefixSum;
RWBuffer<uint>_LocalPrefixSum;
RWBuffer<uint>_GroupKeyOffset;
StructuredBuffer<uint2> DebugBuffer;
//groupShared memory
groupshared uint _GroupBucket[BUCKET_SIZE];
groupshared uint _GroupBinMarkers[BUCKET_SIZE * SORT_SECTION_SIZE_2D];
groupshared uint _GroupContainer[SORT_SECTION_SIZE_2D];

//Output
RWStructuredBuffer<uint2> Result;
//SV_GroupID=(2,1,0)
//SV_DispatchThreadID =global ID
//SV_GroupIndex = flaten Index in Group
//SV_GroupThreadID =uint3(7,5,0)


[numthreads(SORT_SECTION_SIZE, SORT_SECTION_SIZE, 1)]
void CheckOrder(uint groupIdx : SV_GroupIndex, uint3 groupId : SV_GroupID)
{
	uint flatIdx = groupIdx + groupId.x * SORT_SECTION_SIZE_2D;
	if (flatIdx >= _ParticleNum - 1)
		return;

	if (_IndexMap[flatIdx].x > _IndexMap[flatIdx + 1].x)
		InterlockedAnd(_Ordered[0], 0);
}


[numthreads(SORT_SECTION_SIZE, SORT_SECTION_SIZE, 1)]
void LocalCount(uint groupIdx:SV_GroupIndex, uint3 groupId : SV_GroupID)
{
	//Clear
	if (groupIdx == 0)
	{
		for (int i = 0; i < BUCKET_SIZE; ++i)
			_GroupBucket[i] = 0;
	}
	if (groupId.x == 0)
		_GroupContainer[groupIdx] = 0;

	[unroll]
	for (int i = 0; i < BUCKET_SIZE; ++i)
		_LocalBinMarkers[groupIdx + i * SORT_SECTION_SIZE_2D +
		groupId.x * BUCKET_SIZE * SORT_SECTION_SIZE_2D] = 0;
	GroupMemoryBarrierWithGroupSync();

	uint flatIdx = groupIdx + groupId.x * SORT_SECTION_SIZE_2D;
	uint bin = 0;
	if (flatIdx < _ParticleNum)
	{

		bin = (_IndexMap[flatIdx].x >> (_RotationRound * BITS_PER_ROTATION)) & BITS_MASK;
		_LocalBinMarkers[groupIdx + bin * SORT_SECTION_SIZE_2D + groupId.x * BUCKET_SIZE * SORT_SECTION_SIZE_2D] = 1;
		InterlockedAdd(_GroupBucket[bin], 1);

	}
	GroupMemoryBarrierWithGroupSync();
	if (groupIdx == 0)
	{
		//compute local prefix sum
		uint counter = 0;
		[unroll]
		for (int i = 0; i < BUCKET_SIZE; ++i)
		{

			uint oldVal = _GroupBucket[i];
			_GroupKeyOffset[BUCKET_SIZE * groupId.x + i] = oldVal;//GroupOffset
			_GroupBucket[i] = counter;
			InterlockedAdd(_GlobalPrefixSum[i], _GroupBucket[i]);
			_LocalPrefixSum[BUCKET_SIZE * groupId.x + i] = _GroupBucket[i];
			counter += oldVal;
		}
	}

}
[numthreads(SORT_SECTION_SIZE,SORT_SECTION_SIZE,BUCKET_SIZE)]
void PrecedeScan(uint3 DTid:SV_GroupThreadID, uint3 groupId : SV_GroupID) {
	uint groupIdx = DTid.x + DTid.y * SORT_SECTION_SIZE;
	uint bin = DTid.z;
	uint bin_group_offset = bin * SORT_SECTION_SIZE_2D;
	//read to shared memory
	_GroupBinMarkers[groupIdx + bin_group_offset] =
		_LocalBinMarkers[groupIdx + bin_group_offset + groupId.x * BUCKET_SIZE * SORT_SECTION_SIZE_2D];
	GroupMemoryBarrierWithGroupSync();
	//parallel scan: up-sweep
	int d = 0;
	[unroll]
	for (d = 0; d < LOG2_SORT_SECTION_SIZE_2D - 1; ++d)
	{
		uint pow_2_d_1 = 1 << (d + 1);
		if ((groupIdx % pow_2_d_1) == 0)
		{
			_GroupBinMarkers[groupIdx + bin_group_offset + pow_2_d_1 - 1] +=
				_GroupBinMarkers[groupIdx + bin_group_offset + (1 << d) - 1];
		}
		GroupMemoryBarrierWithGroupSync();
	}

	//parallel scan: set last element to zero
	if (groupIdx == SORT_SECTION_SIZE_2D - 1)
	{
		_GroupBinMarkers[groupIdx + bin_group_offset] = 0;
	}
	GroupMemoryBarrierWithGroupSync();

	//parallel scan: down-sweep
	[unroll]
	for (d = LOG2_SORT_SECTION_SIZE_2D - 1; d >= 0; --d)
	{
		uint pow_2_d_1 = 1 << (d + 1);
		uint pow_2_d = (1 << d);
		if ((groupIdx % pow_2_d_1) == 0)
		{
			uint old = _GroupBinMarkers[groupIdx + bin_group_offset + pow_2_d - 1];
			_GroupBinMarkers[groupIdx + bin_group_offset + pow_2_d - 1] =
				_GroupBinMarkers[groupIdx + bin_group_offset + pow_2_d_1 - 1];
			_GroupBinMarkers[groupIdx + bin_group_offset + pow_2_d_1 - 1] += old;
		}
		GroupMemoryBarrierWithGroupSync();
	}

	//write back to global
	_LocalBinMarkers[groupIdx + bin_group_offset + groupId.x * BUCKET_SIZE * SORT_SECTION_SIZE_2D] =
	_GroupBinMarkers[groupIdx + bin_group_offset];
}
[numthreads(SORT_SECTION_SIZE, SORT_SECTION_SIZE, 1)]
void LocalSort(uint groupIdx : SV_GroupIndex, uint3 groupId : SV_GroupID)
{
	uint flatIdx = groupIdx + groupId.x * SORT_SECTION_SIZE_2D;
	if (flatIdx < _ParticleNum)
	{
		uint bin = (_IndexMap[flatIdx].x >> (_RotationRound * BITS_PER_ROTATION)) & BITS_MASK;
		uint localPos =
			_LocalPrefixSum[BUCKET_SIZE * groupId.x + bin] +
			_LocalBinMarkers[groupIdx + bin * SORT_SECTION_SIZE_2D + groupId.x * BUCKET_SIZE * SORT_SECTION_SIZE_2D];
		Result[localPos + groupId.x * SORT_SECTION_SIZE_2D] = _IndexMap[flatIdx];
	}

}
[numthreads(BUCKET_SIZE, 1, 1)]
void ComputeGroupKeyOffset(uint groupIdx : SV_GroupIndex, uint3 groupId : SV_GroupID)
{
	uint counter = 0;
	for (uint i = 0; i < _SortSectionNum; ++i)
	{
		uint oldVal = _GroupKeyOffset[groupIdx + i * BUCKET_SIZE];
		_GroupKeyOffset[groupIdx + i * BUCKET_SIZE] = counter;
		counter += oldVal;
	}
}
[numthreads(SORT_SECTION_SIZE, SORT_SECTION_SIZE, 1)]
void GlobalReorder(uint groupIdx : SV_GroupIndex, uint3 groupId : SV_GroupID)
{
	uint flatIdx = groupIdx + groupId.x * SORT_SECTION_SIZE_2D;
	if (flatIdx >= _ParticleNum)
		return;

	uint bin = (_IndexMap[flatIdx].x >> (_RotationRound * BITS_PER_ROTATION)) & BITS_MASK;
	uint outIdx = _GlobalPrefixSum[bin] +
		_GroupKeyOffset[groupId.x * BUCKET_SIZE + bin] +
		//local index
		groupIdx - _LocalPrefixSum[BUCKET_SIZE * groupId.x + bin];

	Result[outIdx] = _IndexMap[flatIdx];
}